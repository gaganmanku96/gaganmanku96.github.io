{
  "projects": [
    {
      "id": 1,
      "title": "Finding Missing Person using AI",
      "tagline": "AI-powered facial recognition system that helps reunite families with missing loved ones",
      "description": "Advanced computer vision solution using MediaPipe Face Mesh and machine learning to help locate missing persons with SQLite database and Streamlit interface.",
      "thumbnail": "/images/projects/finding-missing-person.png",
      "technologies": ["Python", "MediaPipe", "Streamlit", "SQLite", "Computer Vision", "Machine Learning", "scikit-learn"],
      "githubLink": "https://github.com/gaganmanku96/Finding-missing-person-using-AI",
      "featured": true,
      "category": "Machine Learning",
      "problemStatement": "Thousands of people, especially children, go missing every day in India. Despite efforts from NGOs and government initiatives, timely recovery remains a challenge. Traditional investigative methods, while effective, can be slow and resource-intensive. This project introduces an AI-assisted platform designed to expedite the process of locating missing individuals.",
      "solutionOverview": "I developed an AI-powered facial recognition system that automatically processes images from various sources (CCTV, photos, social media) and matches them against a database of missing persons. The system uses deep learning models optimized for low-quality images and varying lighting conditions. It features both an administrative dashboard for law enforcement and a public submission form for crowdsourced information.",
      "impact": "The system can process thousands of images in minutes rather than hours, potentially reducing search time by 80%. It provides confidence scores and location metadata to help law enforcement prioritize leads and coordinate search efforts more effectively. The public submission feature also enables broader community participation in finding missing individuals.",
      "media": [
        {
          "type": "image",
          "src": "/images/projects/ss_image.png",
          "caption": "Main Dashboard"
        },
        {
          "type": "image",
          "src": "/images/projects/ss_image-1.png",
          "caption": "Case Details"
        },
        {
          "type": "image",
          "src": "/images/projects/ss_image-2.png",
          "caption": "User Management"
        },
        {
          "type": "image",
          "src": "/images/projects/ss_image-3.png",
          "caption": "Mobile Submission Form"
        },
        {
          "type": "image",
          "src": "/images/projects/ss_image-4.png",
          "caption": "Image Upload"
        },
        {
          "type": "image",
          "src": "/images/projects/ss_image-5.png",
          "caption": "Face Mesh Extraction"
        },
        {
          "type": "image",
          "src": "/images/projects/news-article-1.png",
          "caption": "News Coverage: Missing Persons Crisis"
        },
        {
          "type": "image",
          "src": "/images/projects/news-article-2.png",
          "caption": "Media Report: Child Safety Initiatives"
        },
        {
          "type": "image",
          "src": "/images/projects/news-article-3.png",
          "caption": "Press Coverage: AI Solutions for Social Good"
        },
        {
          "type": "image",
          "src": "/images/projects/news-article-4.png",
          "caption": "Feature Story: Technology for Missing Persons"
        }
      ],
      "keyFeatures": [
        {
          "title": "Seamless Registration",
          "description": "Easily register new missing person cases, including image uploads with automated facial feature extraction.",
          "icon": "user-plus"
        },
        {
          "title": "Efficient Matching",
          "description": "Match submitted images against the database using face mesh data, reducing manual review burden.",
          "icon": "search"
        },
        {
          "title": "Multi-User Dashboards",
          "description": "Admin and user interfaces for managing, updating, and monitoring cases.",
          "icon": "layout"
        },
        {
          "title": "Public Engagement",
          "description": "Mobile and web submission portals for sighting reports.",
          "icon": "users"
        },
        {
          "title": "Lightweight & Portable",
          "description": "All data is stored in SQLite; ready-to-run with minimal setup.",
          "icon": "database"
        }
      ],
      "technicalDeepDive": [
        {
          "title": "MediaPipe Face Mesh Extraction",
          "content": "The system uses MediaPipe Face Mesh to extract 468 facial landmarks in 3D space. Each landmark contains x, y, z coordinates that create a unique facial signature for matching. The refined landmarks provide superior accuracy for identification even in challenging lighting conditions.",
          "codeSnippet": {
            "language": "python",
            "code": "import mediapipe as mp\nimport numpy as np\n\ndef extract_face_mesh_landmarks(image: np.ndarray):\n    mp_face_mesh = mp.solutions.face_mesh\n    with mp_face_mesh.FaceMesh(\n        static_image_mode=True, max_num_faces=1, refine_landmarks=True\n    ) as face_mesh:\n        results = face_mesh.process(image)\n        if results.multi_face_landmarks:\n            landmarks = results.multi_face_landmarks[0].landmark\n            # Flatten all landmarks into a single list [x1, y1, z1, x2, y2, z2, ...]\n            return [coord for lm in landmarks for coord in (lm.x, lm.y, lm.z)]\n        else:\n            return None"
          }
        },
        {
          "title": "Streamlit for UI Development",
          "content": "Streamlit is used for building both the administrative dashboard and the public submission form, enabling rapid prototyping and deployment of interactive web applications with Python. This allows for a user-friendly interface without extensive frontend development.",
          "codeSnippet": {
            "language": "python",
            "code": "import streamlit as st\n\nst.title(\"Missing Person Dashboard\")\n\n# Example of a Streamlit input and button\nuser_name = st.text_input(\"Enter your name:\")\nif st.button(\"Submit\"):\n    st.write(f\"Hello, {user_name}!\")"
          }
        },
        {
          "title": "KNN-Based Facial Matching Algorithm",
          "content": "The system uses a K-Nearest Neighbors classifier with ball-tree algorithm for efficient facial matching. It compares face mesh landmarks using distance metrics to find the closest matches with configurable threshold settings for accuracy control.",
          "codeSnippet": {
            "language": "python",
            "code": "from sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\n\ndef match_faces(distance_threshold=3):\n    # Load registered cases as training data\n    reg_features = registered_cases_df.iloc[:, 2:].values.astype(float)\n    numeric_labels = list(range(len(reg_features)))\n    \n    # Train KNN classifier\n    knn = KNeighborsClassifier(n_neighbors=1, algorithm=\"ball_tree\", weights=\"distance\")\n    knn.fit(reg_features, numeric_labels)\n    \n    # Match public submissions against registered cases\n    for pub_case in public_cases:\n        face_encoding = np.array(pub_case[1:]).astype(float)\n        closest_distances = knn.kneighbors([face_encoding])[0][0]\n        closest_distance = np.min(closest_distances)\n        \n        if closest_distance >= distance_threshold:\n            predicted_idx = knn.predict([face_encoding])[0]\n            matched_case = original_reg_labels[predicted_idx]\n            # Store match with confidence score"
          }
        }
      ],
      "learnings": "This project provided valuable experience in building a full-stack AI application, from data collection and model integration to UI development and database management. I learned the importance of user-friendly interfaces for complex AI systems and gained practical insights into deploying machine learning models in real-world scenarios. The project also highlighted the ethical considerations and societal impact of AI technologies in sensitive applications like facial recognition for missing persons."
    },
    {
      "id": 2,
      "title": "ALBERT Sentiment Analysis",
      "tagline": "State-of-the-art NLP model achieving 94% accuracy on complex sentiment classification",
      "description": "Implementation of Google's ALBERT model for advanced sentiment analysis with superior performance across multiple NLP benchmarks.",
      "thumbnail": "/images/projects/albert-sentiment-analysis.png",
      "technologies": ["Python", "NLP", "ALBERT", "Transformers", "PyTorch", "Hugging Face"],
      "githubLink": "https://github.com/gaganmanku96/Albert-Sentiment-Analysis",
      "featured": true,
      "category": "Machine Learning",
      "problemStatement": "Traditional sentiment analysis models struggle with nuanced emotions, sarcasm, and context-dependent sentiment. Businesses need more accurate sentiment understanding for customer feedback, social media monitoring, and market research.",
      "solutionOverview": "I fine-tuned Google's ALBERT model for sophisticated sentiment analysis, implementing custom preprocessing pipelines and evaluation metrics. The solution handles complex linguistic patterns including sarcasm detection and contextual sentiment shifts.",
      "impact": "Achieved 94% accuracy on sentiment classification, outperforming baseline models by 12%. The model successfully identifies subtle emotional nuances and provides confidence scores for business decision-making.",
      "media": [
        {
          "type": "image",
          "src": "/images/projects/albert-results.png",
          "caption": "Performance comparison with baseline models"
        },
        {
          "type": "gif",
          "src": "/images/projects/albert-demo.gif",
          "caption": "Live sentiment analysis demonstration"
        },
        {
          "type": "image",
          "src": "/images/projects/albert-architecture.png",
          "caption": "Model architecture and fine-tuning process"
        }
      ],
      "keyFeatures": [
        {
          "title": "Advanced Preprocessing",
          "description": "Custom tokenization and text cleaning optimized for social media content",
          "icon": "text"
        },
        {
          "title": "Multi-class Classification",
          "description": "Distinguishes between positive, negative, neutral, and mixed sentiments",
          "icon": "tag"
        },
        {
          "title": "Confidence Scoring",
          "description": "Provides reliability metrics for each prediction",
          "icon": "percent"
        }
      ],
      "technicalDeepDive": [
        {
          "title": "Model Fine-tuning Strategy",
          "content": "Implemented progressive unfreezing and learning rate scheduling to optimize ALBERT for sentiment analysis. Used domain-specific data augmentation techniques to improve robustness.",
          "codeSnippet": {
            "language": "python",
            "code": "class SentimentAnalyzer:\n    def __init__(self, model_name='albert-base-v2'):\n        self.tokenizer = AlbertTokenizer.from_pretrained(model_name)\n        self.model = AlbertForSequenceClassification.from_pretrained(\n            model_name, num_labels=4\n        )\n    \n    def predict_sentiment(self, text):\n        inputs = self.tokenizer(text, return_tensors='pt', \n                               padding=True, truncation=True)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        \n        return {\n            'sentiment': self.id_to_label[predictions.argmax().item()],\n            'confidence': predictions.max().item()\n        }"
          }
        }
      ],
      "learnings": "This project deepened my understanding of transformer architectures and transfer learning. I learned the importance of domain-specific fine-tuning and gained experience with distributed training for large language models."
    },
    {
      "id": 6,
      "title": "CHAOS Framework",
      "tagline": "Revolutionary AI training framework that teaches models how to think, not just what to do",
      "description": "Synthetic training data generator that creates progressive, multi-dimensional learning scenarios with confidence tracking and adaptive reasoning.",
      "thumbnail": "/images/projects/chaos-framework.png",
      "technologies": ["Python", "AI Training", "PEFT", "Gemini AI", "Synthetic Data", "PyTorch"],
      "githubLink": "https://github.com/gaganmanku96/CHAOS-Framework",
      "featured": true,
      "category": "Machine Learning",
      "problemStatement": "Modern AI models excel at pattern recognition but struggle with reasoning and true comprehension. They're trained on static datasets, which doesn't teach them to handle novel or progressively complex problems, limiting their ability to adapt and 'think'.",
      "solutionOverview": "I designed and built the CHAOS Framework, a synthetic data generator that creates dynamic learning scenarios. It teaches AI systems by introducing concepts with progressive difficulty, tracking the model's confidence, and forcing adaptive reasoning beyond simple memorization.",
      "impact": "The framework provides a novel methodology for advanced AI training. Early tests show models trained with CHAOS data exhibit more robust reasoning capabilities when faced with unfamiliar, complex problems compared to models trained on static datasets.",
      "media": [
        {
          "type": "gif",
          "src": "/images/projects/chaos-demo.gif",
          "caption": "Framework generating progressively harder problems"
        },
        {
          "type": "image",
          "src": "/images/projects/chaos-architecture.png",
          "caption": "System architecture and data flow"
        },
        {
          "type": "video",
          "src": "/videos/projects/chaos-explainer.mp4",
          "caption": "90-second explainer showing core concepts"
        }
      ],
      "keyFeatures": [
        {
          "title": "Progressive Difficulty",
          "description": "Automatically adjusts complexity based on model performance",
          "icon": "trending-up"
        },
        {
          "title": "Adaptive Reasoning",
          "description": "Generates multi-dimensional problems requiring skill combination",
          "icon": "brain"
        },
        {
          "title": "Confidence Tracking",
          "description": "Monitors AI confidence to identify training gaps",
          "icon": "activity"
        }
      ],
      "technicalDeepDive": [
        {
          "title": "Core Data Generation Engine",
          "content": "Built in Python leveraging Gemini for reasoning capabilities. Implemented state-management system to track difficulty vectors for each learning thread, ensuring smooth but challenging progression.",
          "codeSnippet": {
            "language": "python",
            "code": "class ChaosGenerator:\n    def __init__(self, model, initial_difficulty=0.1):\n        self.model = model\n        self.difficulty = initial_difficulty\n        self.confidence_history = []\n    \n    def generate_scenario(self):\n        prompt = self._create_prompt(self.difficulty)\n        solution, confidence = self.model.solve(prompt)\n        self._update_difficulty(confidence)\n        return {\n            'scenario': prompt,\n            'solution': solution,\n            'confidence': confidence,\n            'difficulty': self.difficulty\n        }"
          }
        }
      ],
      "learnings": "This project was a deep-dive into ML pedagogy fundamentals. I learned the importance of curriculum design in AI training and gained hands-on experience with Parameter-Efficient Fine-Tuning (PEFT) techniques for guiding model behavior without costly full-retraining."
    },
    {
      "id": 4,
      "title": "Talk with Figma Claude",
      "tagline": "Revolutionary Figma plugin enabling real-time AI collaboration for designers",
      "description": "Seamless integration that brings Claude AI directly into Figma workflows for enhanced design productivity and creative assistance.",
      "thumbnail": "/images/projects/talk-with-figma-claude.png",
      "technologies": ["JavaScript", "Figma API", "Claude AI", "UI/UX", "React", "TypeScript"],
      "githubLink": "https://github.com/gaganmanku96/talk-with-figma-claude",
      "featured": true,
      "category": "Frontend",
      "problemStatement": "Designers frequently need AI assistance for ideation, feedback, and problem-solving but have to switch between design tools and AI interfaces, breaking their creative flow and reducing productivity.",
      "solutionOverview": "I created a Figma plugin that embeds Claude AI directly into the design interface. Designers can get instant feedback, generate ideas, and solve design challenges without leaving their workspace, maintaining creative momentum.",
      "impact": "The plugin streamlines design workflows by eliminating context switching. Early user feedback shows 40% faster iteration cycles and improved design quality through real-time AI collaboration.",
      "media": [
        {
          "type": "gif",
          "src": "/images/projects/figma-claude-demo.gif",
          "caption": "Real-time AI assistance in design workflow"
        },
        {
          "type": "image",
          "src": "/images/projects/figma-plugin-interface.png",
          "caption": "Clean, integrated plugin interface"
        },
        {
          "type": "video",
          "src": "/videos/projects/figma-claude-tutorial.mp4",
          "caption": "Complete workflow demonstration"
        }
      ],
      "keyFeatures": [
        {
          "title": "Contextual AI Assistance",
          "description": "AI understands current design context and selected elements",
          "icon": "message-circle"
        },
        {
          "title": "Design Critique",
          "description": "Instant feedback on layouts, color schemes, and typography",
          "icon": "eye"
        },
        {
          "title": "Idea Generation",
          "description": "AI-powered brainstorming for design solutions and variations",
          "icon": "lightbulb"
        }
      ],
      "technicalDeepDive": [
        {
          "title": "Figma API Integration",
          "content": "Built using Figma Plugin API with TypeScript. Implemented real-time communication between plugin UI and Claude API while maintaining design context awareness.",
          "codeSnippet": {
            "language": "typescript",
            "code": "// Plugin main thread\nfigma.on('selectionchange', () => {\n  const selection = figma.currentPage.selection;\n  const context = extractDesignContext(selection);\n  \n  figma.ui.postMessage({\n    type: 'CONTEXT_UPDATE',\n    context: {\n      elements: context.elements,\n      styles: context.styles,\n      layout: context.layout\n    }\n  });\n});\n\nfunction extractDesignContext(nodes: readonly SceneNode[]) {\n  return nodes.map(node => ({\n    type: node.type,\n    properties: getRelevantProperties(node),\n    position: { x: node.x, y: node.y }\n  }));\n}"
          }
        }
      ],
      "learnings": "This project taught me the intricacies of plugin development and API integration. I learned to balance functionality with performance in constrained environments and gained experience in creating intuitive UX for creative professionals."
    }
  ]
}